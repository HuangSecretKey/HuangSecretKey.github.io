---
title: CNN中的等变(equivariant)和不变(invariant)
date: 2021-01-03 21:16:26
tags: -深度学习
---

### 等变性equivariant

通俗解释： 对于一个函数，如果你对其输入施加的变换也会同样反应在输出上，那么这个函数就对该变换具有等变性。
更严谨些：
对于一个函数/特征f以及一个变换g， 如果我们有：

f(g(x))=g(f(x))

则称f对变换g有等变性。
举一个例子，假设我们的变换g是将图像向右平移一段距离，我们的函数f是检测一个人脸的位置（比如说输出坐标），f(g(x))就是先将图片像右移，接着我们在新图较之原图偏右的位置检测到人脸； g(f(x))则是我们先检测到人脸， 然后再将人脸往右移一点。这二者的输出是一样的，与我们施加变换的顺序无关。
<!-- more -->

### 不变性 invraiant
通俗解释： 对于一个函数，如果对其输入施加的某种操作丝毫不会影响到输出，那么这个函数就对该变换具有不变性。
更严谨些：
假设我们的输入为x，函数为f, 此时我们先对输入做变换g: g(x)=x′，此时若有：

f(x)=f(x′)=f(g(x))

则称f对变换g具有不变性。
举个例子我们的函数是检测图像中是否有红色， 此时如果我们的变换是旋转/平移， 那么这些变换都不会对函数结果有任何影响， 就可以说该函数对该变换具有不变性

### 等变、不变 与 CNN

那么说到底，equivariant和invariant和卷积神经网络到底有啥关系呢？
简单的来说，CNN中的卷积操作中的参数共享使得它对平移操作有等变性，而一些池化操作对平移有近似不变性。
先来说前者， 我们举个很简单的例子，我们都知道CNN的第一层往往可以解释为一些简单的线条处理，比如竖直/水平线条检测等等，那么如果图像平移，显然并不会影响到这一层线条检测的功能，但是其输出也会做相应平移。
后者的之所以说是近似不变性，是因为池化层并非能保持完全不变，例如我们使用max池化，只要变换不影响到最大值，我们的池化结果不会收到影响，对于一个NxN的filter，只有一个值的变动会影响到输出， 其他的变换都不会造成扰动。 平均池化的近似不变性就稍弱些。这里池化的其实是一个非常强的先验，等于是忽视了这一步维数约简带来的信息损失而保证了近似不变性。

我们换个角度来说，CNN是既具有不变性，又具有等变性。 可以这么理解，如果我们的输出是给出图片中猫的位置，那么我们将图片中的猫从左边移到右边，这种平移也会反应在输出上，我们输出的位置也是从左边到右边，那么我们则可以说CNN有等变性；如果我们只是输出图片中是否有猫，那么我们无论把猫怎么移动，我们的输出都保持”有猫”的判定，因此体现了CNN的不变性。